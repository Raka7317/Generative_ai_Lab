{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6TMRnkzXxMzSRTtPjyHqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raka7317/Generative_ai_Lab/blob/main/BIGRAM_TRIGRAM_PART2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import re\n",
        "import math\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Converts text to lowercase and removes punctuation.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Splits text into words.\"\"\"\n",
        "    return text.split()\n",
        "\n",
        "def generate_ngrams(tokens, n):\n",
        "    \"\"\"Generates n-grams from a list of tokens.\"\"\"\n",
        "    if len(tokens) < n:\n",
        "        return []\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        ngrams.append(tuple(tokens[i:i+n]))\n",
        "    return ngrams\n",
        "\n",
        "def ngram_prob_no_smoothing(ngrams_list, prefixes_list):\n",
        "    \"\"\"Calculates n-gram probabilities without smoothing.\n",
        "\n",
        "    Args:\n",
        "        ngrams_list: A list of n-grams (e.g., [('a', 'b'), ('b', 'c')]).\n",
        "        prefixes_list: A list of (n-1)-gram prefixes (e.g., [('a',), ('b',)]).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping n-gram to its probability.\n",
        "    \"\"\"\n",
        "    ngram_counts = collections.Counter(ngrams_list)\n",
        "    prefix_counts = collections.Counter(prefixes_list)\n",
        "\n",
        "    probabilities = {}\n",
        "    for ngram in ngrams_list:\n",
        "        prefix = ngram[:-1]\n",
        "\n",
        "        ngram_count = ngram_counts[ngram]\n",
        "        prefix_count = prefix_counts[prefix]\n",
        "\n",
        "        if prefix_count == 0:\n",
        "            probabilities[ngram] = 0.0 # If prefix count is 0, probability is 0\n",
        "        else:\n",
        "            probabilities[ngram] = ngram_count / prefix_count\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "def ngram_prob_laplace(ngrams_list, prefixes_list, vocab_size):\n",
        "    \"\"\"Calculates n-gram probabilities with Laplace smoothing.\n",
        "\n",
        "    Args:\n",
        "        ngrams_list: A list of n-grams.\n",
        "        prefixes_list: A list of (n-1)-gram prefixes.\n",
        "        vocab_size: The size of the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping n-gram to its smoothed probability.\n",
        "    \"\"\"\n",
        "    ngram_counts = collections.Counter(ngrams_list)\n",
        "    prefix_counts = collections.Counter(prefixes_list)\n",
        "\n",
        "    probabilities = {}\n",
        "\n",
        "    for ngram in ngrams_list:\n",
        "        prefix = ngram[:-1]\n",
        "\n",
        "        ngram_count = ngram_counts[ngram]\n",
        "        prefix_count = prefix_counts[prefix]\n",
        "\n",
        "        # Laplace smoothing: (C(ngram) + 1) / (C(prefix) + V)\n",
        "        probabilities[ngram] = (ngram_count + 1) / (prefix_count + vocab_size)\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "def calculate_perplexity(ngrams, probabilities):\n",
        "    \"\"\"Calculates the perplexity of a sequence of n-grams.\n",
        "\n",
        "    Args:\n",
        "        ngrams: The sequence of n-grams (e.g., test set).\n",
        "        probabilities: A dictionary mapping n-grams to their probabilities.\n",
        "\n",
        "    Returns:\n",
        "        The perplexity as a float.\n",
        "    \"\"\"\n",
        "    log_sum_prob = 0.0\n",
        "    N = len(ngrams)\n",
        "\n",
        "    if N == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    for ngram in ngrams:\n",
        "        prob = probabilities.get(ngram)\n",
        "        if prob is None or prob == 0:\n",
        "            # If an n-gram has 0 probability, perplexity becomes infinite.\n",
        "            return float('inf')\n",
        "        log_sum_prob += math.log(prob)\n",
        "\n",
        "    # Perplexity = (1 / P(W))^(1/N) = exp(-1/N * sum(log P(wi)))\n",
        "    perplexity = math.exp(-log_sum_prob / N)\n",
        "    return perplexity\n",
        "\n",
        "text = \"Data science is fun and data science is powerful\"\n",
        "\n",
        "# Step 1: Preprocess\n",
        "clean_text = preprocess_text(text)\n",
        "\n",
        "# Step 2: Tokenize\n",
        "tokens = tokenize(clean_text)\n",
        "vocab_size = len(set(tokens))\n",
        "\n",
        "# ---------------- BIGRAM ----------------\n",
        "bigrams = generate_ngrams(tokens, 2)\n",
        "unigrams = [(token,) for token in tokens]\n",
        "\n",
        "# Without smoothing\n",
        "bigram_probs_no = ngram_prob_no_smoothing(bigrams, unigrams)\n",
        "bigram_pp_no = calculate_perplexity(bigrams, bigram_probs_no)\n",
        "\n",
        "# With Laplace smoothing\n",
        "bigram_probs_la = ngram_prob_laplace(bigrams, unigrams, vocab_size)\n",
        "bigram_pp_la = calculate_perplexity(bigrams, bigram_probs_la)\n",
        "\n",
        "# ---------------- TRIGRAM ----------------\n",
        "trigrams = generate_ngrams(tokens, 3)\n",
        "bigram_prefixes = generate_ngrams(tokens, 2)\n",
        "\n",
        "# Without smoothing\n",
        "trigram_probs_no = ngram_prob_no_smoothing(trigrams, bigram_prefixes)\n",
        "trigram_pp_no = calculate_perplexity(trigrams, trigram_probs_no)\n",
        "\n",
        "# With Laplace smoothing\n",
        "trigram_probs_la = ngram_prob_laplace(trigrams, bigram_prefixes, vocab_size)\n",
        "trigram_pp_la = calculate_perplexity(trigrams, trigram_probs_la)\n",
        "\n",
        "# ---------------- OUTPUT ----------------\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "print(\"\\nBIGRAM PERPLEXITY\")\n",
        "print(\"Without smoothing:\", bigram_pp_no)\n",
        "print(\"With Laplace smoothing:\", bigram_pp_la)\n",
        "\n",
        "print(\"\\nTRIGRAM PERPLEXITY\")\n",
        "print(\"Without smoothing:\", trigram_pp_no)\n",
        "print(\"With Laplace smoothing:\", trigram_pp_la)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "kOiQDJSaz-AF",
        "outputId": "fbd50d8a-ac67-41c8-cdb9-d4b94ac60e3b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ngram_prob_no_smoothing' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-831494546.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Without smoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbigram_probs_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram_prob_no_smoothing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mbigram_pp_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_probs_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ngram_prob_no_smoothing' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# -----------------------------\n",
        "# 1. TEXT PREPROCESSING\n",
        "# -----------------------------\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2. TOKENIZATION\n",
        "# -----------------------------\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3. GENERATE N-GRAMS\n",
        "# -----------------------------\n",
        "def generate_ngrams(tokens, n):\n",
        "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4. PROBABILITY WITHOUT SMOOTHING\n",
        "# -----------------------------\n",
        "def ngram_prob_no_smoothing(ngrams, lower_ngrams):\n",
        "    ngram_count = Counter(ngrams)\n",
        "    lower_count = Counter(lower_ngrams)\n",
        "\n",
        "    probs = {}\n",
        "    for ngram in ngram_count:\n",
        "        prefix = ngram[:-1]\n",
        "        probs[ngram] = ngram_count[ngram] / lower_count[prefix]\n",
        "    return probs\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5. PROBABILITY WITH LAPLACE SMOOTHING\n",
        "# -----------------------------\n",
        "def ngram_prob_laplace(ngrams, lower_ngrams, vocab_size):\n",
        "    ngram_count = Counter(ngrams)\n",
        "    lower_count = Counter(lower_ngrams)\n",
        "\n",
        "    probs = {}\n",
        "    for ngram in ngram_count:\n",
        "        prefix = ngram[:-1]\n",
        "        probs[ngram] = (ngram_count[ngram] + 1) / (lower_count[prefix] + vocab_size)\n",
        "    return probs\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6. PERPLEXITY CALCULATION\n",
        "# -----------------------------\n",
        "def calculate_perplexity(ngrams, probabilities):\n",
        "    N = len(ngrams)\n",
        "    log_sum = 0\n",
        "\n",
        "    for ngram in ngrams:\n",
        "        prob = probabilities.get(ngram, 1e-10)  # avoid log(0)\n",
        "        log_sum += math.log(prob)\n",
        "\n",
        "    return math.exp(-log_sum / N)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 7. MAIN EXECUTION\n",
        "# -----------------------------\n",
        "text = \"Data science is fun and data science is powerful\"\n",
        "\n",
        "# Preprocessing\n",
        "clean_text = preprocess_text(text)\n",
        "\n",
        "# Tokenization\n",
        "tokens = tokenize(clean_text)\n",
        "vocab_size = len(set(tokens))\n",
        "\n",
        "# -------- BIGRAM --------\n",
        "bigrams = generate_ngrams(tokens, 2)\n",
        "unigrams = [(token,) for token in tokens]\n",
        "\n",
        "bigram_probs_no = ngram_prob_no_smoothing(bigrams, unigrams)\n",
        "bigram_pp_no = calculate_perplexity(bigrams, bigram_probs_no)\n",
        "\n",
        "bigram_probs_la = ngram_prob_laplace(bigrams, unigrams, vocab_size)\n",
        "bigram_pp_la = calculate_perplexity(bigrams, bigram_probs_la)\n",
        "\n",
        "# -------- TRIGRAM --------\n",
        "trigrams = generate_ngrams(tokens, 3)\n",
        "bigram_prefixes = generate_ngrams(tokens, 2)\n",
        "\n",
        "trigram_probs_no = ngram_prob_no_smoothing(trigrams, bigram_prefixes)\n",
        "trigram_pp_no = calculate_perplexity(trigrams, trigram_probs_no)\n",
        "\n",
        "trigram_probs_la = ngram_prob_laplace(trigrams, bigram_prefixes, vocab_size)\n",
        "trigram_pp_la = calculate_perplexity(trigrams, trigram_probs_la)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 8. OUTPUT\n",
        "# -----------------------------\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "print(\"\\nBIGRAM PERPLEXITY\")\n",
        "print(\"Without smoothing:\", bigram_pp_no)\n",
        "print(\"With Laplace smoothing:\", bigram_pp_la)\n",
        "\n",
        "print(\"\\nTRIGRAM PERPLEXITY\")\n",
        "print(\"Without smoothing:\", trigram_pp_no)\n",
        "print(\"With Laplace smoothing:\", trigram_pp_la)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR_LL1KK0Qnm",
        "outputId": "852a19c3-6228-4e11-aa30-d1f3f4b98aff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['data', 'science', 'is', 'fun', 'and', 'data', 'science', 'is', 'powerful']\n",
            "\n",
            "BIGRAM PERPLEXITY\n",
            "Without smoothing: 1.189207115002721\n",
            "With Laplace smoothing: 3.158758147025058\n",
            "\n",
            "TRIGRAM PERPLEXITY\n",
            "Without smoothing: 1.2190136542044754\n",
            "With Laplace smoothing: 3.364298418765503\n"
          ]
        }
      ]
    }
  ]
}